"""
Ce script permet de run complÃ©tement notre pipeline ETL avec les Ã©tapes suivantes :
1. Extraire les donnÃ©es utilisateurs du fichier CSV
2. Nettoyer et transformer les donnÃ©es
3. Charger les donnÃ©es dans la database PostgreSQL 

A run avec: python main.py
"""

### Modifications Ã  faire selon votre environnement

"""
Dans generate_data.py, modifier la ligne 90 pour enregistrer le CSV dans le bon dossier de votre environnement 
"""

from data.generate_data import generate_data
from etl.clean_survey_data import clean
from etl.load_data import load_to_database, test_database_connection

def main():
    print("ğŸš€ DÃ©marrage du pipeline ETL...")
    print("=" * 50)

    # Etape 1 : GÃ©nÃ©rer les donnÃ©es
    print("\n=== GENERATION ===")
    generate_data()

    print("âœ… GÃ©nÃ©ration des donnÃ©es terminÃ©e. Voir les fichiers users_profiles et users_travels")

    # Etape 2 : Nettoyer les donnÃ©es
    print("\n=== NETTOYAGE ===")
    clean()

    print("âœ… Nettoyage des donnÃ©es terminÃ©e. Voir les fichiers data_profiles_clean et data_travels_clean")

    # Etape 3 : Charger les donnÃ©es dans la base de donnÃ©es
    print("\n=== CHARGEMENT ===")
    load_to_database("data/data_profiles_clean.csv", table_name="users_profiles")
    load_to_database("data/data_travels_clean.csv", table_name="users_travels")

    print("âœ… Chargement des donnÃ©es terminÃ©.")

    # Etape 4 : VÃ©rifier le chargement
    print("\n=== VERIFICATION ===")
    test_database_connection()

    print("\nğŸ‰ ETL Pipeline completed!")
    print("=" * 50)

main()